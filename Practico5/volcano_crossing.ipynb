{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr치ctico 5: Volcano Crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcano_crossing_env import VolcanoCrossing\n",
    "\n",
    "env = VolcanoCrossing(slip_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jugar manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game():\n",
    "        print(\"Play manually...\")\n",
    "        obs = env.reset()\n",
    "        print(obs)\n",
    "        done = False\n",
    "        step_counter = 0\n",
    "        all_rewards = 0\n",
    "        env.render()\n",
    "\n",
    "        while not done:\n",
    "            action = input(\"Next action: \")\n",
    "            env.check_action(action)\n",
    "            obs, reward, done_env, _ = env.step(action)\n",
    "            print(f'{obs=} {reward=} {done_env=}')\n",
    "            all_rewards += reward\n",
    "            done = done_env\n",
    "            env.render()\n",
    "            step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jugar con una policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_south(state) :\n",
    "  return 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(policy) :\n",
    "  U = 0\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  while not done:\n",
    "    state, reward, done, _ = env.step(policy[state])\n",
    "    U += reward\n",
    "  return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimaci칩n de la policy por promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: -2.50908\n"
     ]
    }
   ],
   "source": [
    "r = 0\n",
    "N = 50_000\n",
    "for i in range(N):\n",
    "    r += run(policies)\n",
    "\n",
    "print (f'Average reward: {r/N}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, delta = 0.05, gamma = 0.999):\n",
    "  V = { '11':0, '12':0, '13':0, '14':0, '21':0, '22':0, '23':0, '24':0, '31':0, '32':0, '33':0, '34':0 }\n",
    "  V_new = { '11':0, '12':0, '13':0, '14':0, '21':0, '22':0, '23':0, '24':0, '31':0, '32':0, '33':0, '34':0 }\n",
    "  max_diff = delta + 1\n",
    "\n",
    "  while max_diff > delta:\n",
    "    for s in env.observation_space:\n",
    "      a = policy[s]\n",
    "      q = 0\n",
    "      for s_prime in env.P[s][a].keys():\n",
    "        q += env.P[s][a][s_prime] * (env.R[s][a][s_prime] + gamma * V[s_prime])\n",
    "      V_new[s] = q\n",
    "    max_diff = 0\n",
    "    for s in V.keys():\n",
    "      max_diff = max(max_diff, abs(V[s]-V_new[s]))\n",
    "    V = V_new.copy()\n",
    "  return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': -10.097479836569216, '12': -25.02876509699797, '13': 0.0, '14': 0.0, '21': -5.153966059429463, '22': -21.73443636193771, '23': 0.0, '24': -21.539534697673236, '31': 0.0, '32': -16.790727065526983, '33': -30.83304348111256, '34': -25.988180160178665}\n"
     ]
    }
   ],
   "source": [
    "south_policy = { '11':'S', '12':'S', '13':'S', '14':'S', '21':'S', '22':'S', '23':'S', '24':'S', '31':'S', '32':'S', '33':'S', '34':'S' }\n",
    "V = policy_evaluation(south_policy, delta = 0.05, gamma = 0.999)\n",
    "print(V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = { '11':'S', '12':'S', '13':'S', '14':'S', '21':'S', '22':'S', '23':'S', '24':'S', '31':'S', '32':'S', '33':'S', '34':'S' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar un paso de policy improvement\n",
    "def policy_improvement(V,policies):\n",
    "    actions = ['N', 'S', 'E', 'W']\n",
    "    for s in env.observation_space:\n",
    "        q = {a: 0 for a in actions}\n",
    "        for a in actions:\n",
    "            for s_prime in env.P[s][a].keys():\n",
    "                q[a] += env.P[s][a][s_prime] * (env.R[s][a][s_prime] + 0.999 * V[s_prime])\n",
    "        policies[s] = max(q, key=q.get)\n",
    "    return policies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final policy:\n",
      "State 11: S\n",
      "State 12: W\n",
      "State 13: N\n",
      "State 14: N\n",
      "State 21: S\n",
      "State 22: W\n",
      "State 23: N\n",
      "State 24: N\n",
      "State 31: N\n",
      "State 32: W\n",
      "State 33: W\n",
      "State 34: N\n"
     ]
    }
   ],
   "source": [
    "# Realizar policy iteration y ver cu치l es la mejor acci칩n para cada estado\n",
    "from collections import defaultdict\n",
    "\n",
    "policies = { '11':'S', '12':'S', '13':'S', '14':'S', '21':'S', '22':'S', '23':'S', '24':'S', '31':'S', '32':'S', '33':'S', '34':'S' }\n",
    "policies_eval_value = policy_evaluation(policies, delta = 0.05, gamma = 0.999)\n",
    "while True:\n",
    "    new_pol = policy_improvement(V, policies)\n",
    "    new_pol_eval_value = policy_evaluation(new_pol, delta = 0.05, gamma = 0.999)\n",
    "    if new_pol_eval_value == policies_eval_value:\n",
    "        break\n",
    "    policies = new_pol.copy()\n",
    "    policies_eval_value = new_pol_eval_value.copy()\n",
    "    \n",
    "print(\"Final policy:\")\n",
    "for s in policies.keys():\n",
    "    print(f\"State {s}: {policies[s]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
